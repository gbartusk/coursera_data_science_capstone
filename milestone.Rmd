---
title: "Coursera Data Science Capstone - Milestone Report"
date: "July 2015"
output: html_document
---

### Executive Summary
This report explores pre-processing and exploratory analysis techniques on English language text. Ultimately this analysis will flow into a predictive text algorithm and corresponding Shiny app.  

The analysis is laid out across four main sections. The start of each section describes the methods employed followed by outputs / graphs; all code is shown to allow for reproducibility.

Report Sections and Key Findings:  

* _Data Loading_:
    + The raw data files are over half a GB on disk and contains more than 4mm lines
* _Data Cleaning_:
    + Due to the wide range of text sources, the data requires a lot of pre-processing
    + Cleaning techniques include removing foreign character and punctuation, profanity filtering, etc
* _Exploratory Analysis_:
    + The most frequent letter is 'e'
    + The frequency of individual words is highly skewed towards a subset of words
    + The most frequent single words are: 'said', 'will', 'one'
    + The most frequent 2 word sequences are: 'last night', 'new york', 'right now'
    + The most frequent 3 word sequences are: 'new york city', 'two years ago', 'let us know'
    + Coverage: as the length of the n-gram increases, the skewness fades and it takes an increasingly proportional percentage of n-grams to cover the same percentage of text
* _Future Development_:
    + Build a predictive language model
    + Deploy the predictive model via a Shiny app

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```


### Data Loading + Setup
The data sets are provided by HC Corpora (www.corpora.heliohost.org) and consist of US English text documents pulled from online newspapers, blogs, and Twitter entries. The raw data files are converted to tm corpus objects which provides a framework (set of methods) for cleaning, manipulating, and analyzing the data.  

```{r load_pkgs, echo=TRUE, message=FALSE}
library(knitr)          # - report generation
library(NLP)            # - nlp infrastructure (widely used by other pkgs)
library(tm)             # - comprehensive text mining framework
library(qdap)           # - quantitative discourse analysis
library(pander)         # - print tables
library(ggplot2)        # - all-purpose graphing 
library(dplyr)          # - data manipulation
library(gridExtra)      # - panel plots
```

```{r load_data, cache=TRUE}
# - define document ids
id_news <- "en_US.news.txt"
id_blog <- "en_US.blogs.txt"
id_twit <- "en_US.twitter.txt"

# - define file paths to documents on disk
sample_dir <- file.path("sample_data", "en_US")
data_dir <- file.path("final", "en_US")
file_news <- file.path(data_dir, id_news)
file_blog <- file.path(data_dir, id_blog)
file_twit <- file.path(data_dir, id_twit)

# - load documents into corpus object
corpus_all <- tm::Corpus(tm::DirSource(sample_dir, encoding="UTF-8"), 
    readerControl = list(language="en_US"))

# - load vector of profanity (source: http://fffff.at/googles-official-list-of-bad-words/)
file_profanity <- file.path("profanity_list_en.txt")
dat_profanity <- readLines(file_profanity)
```

```{r summarise_files, cache=TRUE}
# - pull number of lines, word count, and char cound using the nix wc command
nix_wc <- function(filepath)
{
    out <- system(paste("wc -lwc",filepath,"| awk {'print $1, $2, $3'}"), intern=TRUE)
    vec <- as.numeric(unlist(strsplit(out," ")))
    return(list('lines'=vec[1], 'words'=vec[2], 'chars'=vec[3]))
}

# - grab file stats
wc_news <- nix_wc(file_news)
wc_blog <- nix_wc(file_blog)
wc_twit <- nix_wc(file_twit)
 
# - data frame summarising the files
df_sum <- data.frame(
    "Data Source" = c("Newspapers", "Blog", "Twitter", "Total"),
    "File Size (MB)" = c(file.size(c(file_news, file_blog, file_twit))/1000^2, 0),
    "Lines" = c(wc_news$lines, wc_blog$lines, wc_twit$lines, 0),
    "Words" = c(wc_news$words, wc_blog$words, wc_twit$words, 0),
    "Characters" = c(wc_news$chars, wc_blog$chars, wc_twit$chars, 0),
    check.names = FALSE
)
# - compute totals row
df_sum[4,2:5] <- colSums(df_sum[,-1])
```

```{r summary_tbl}
# - file summary table
pander::panderOptions("round", 2)
pander::panderOptions("big.mark", ",")
pander::panderOptions("table.split.table", Inf)
pander::pander(df_sum)
```


### Data Cleaning + Term Document Matrix
First the data will be cleaned into a unified format so it can processed into distinct words and accurately analyzed. Then a 'term document matrix' will be created which consists of documents (news, blog, Twitter) as columns and distinct words as rows. The term document matrix is a common format for representing texts for computations.  

The following cleaning is performed on the corpus:  

* _Foreign Characters_: remove all non-ASCII characters (eg: accented letters, emoticons, etc)
* _Lower Case_: treat words as equal regardless of case  (eg: Dog = dog)
* _Stop Words_: remove common words that don't provide much value for analysis  (eg: to, a, is)
* _Punctuation_: strip all punctuation
* _Profanity_: remove profanity since they shouldn't be predicted 
* _Numbers_: remove numbers as only interested in predicting words 
* _Whitespace_: strip all white space so words are nicely separated
* _Future Considerations_: stemming words to their root form, correcting common spelling mistakes, mapping foreign characters onto ASCII equivalents 

```{r data_clean_tdm, cache=TRUE}
# - define function to clean documents
clean_corpus <- function(corpus)
{
    # - tranformer: gsub replacement
    trans_gsub <- tm::content_transformer(
        function(x, pattern, replacement) gsub(pattern, replacement, x))
    # - transformer: remove unicode characters
    rm_unicode <- tm::content_transformer(
        function(x) iconv(x, from="UTF-8", to="ASCII", sub=""))
    # - remove unicode characters
    corpus <- tm::tm_map(corpus, rm_unicode)
    # - conversion to lower case
    corpus <- tm::tm_map(corpus, tm::content_transformer(tolower))
    # - remove stop words
    corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
    # - remove punctuation
    corpus <- tm::tm_map(corpus, trans_gsub, "[[:punct:]]", "")
    # - remove profanity (perform after lowercase)
    corpus <- tm::tm_map(corpus, tm::removeWords, dat_profanity)
    # - remove numbers
    corpus <- tm::tm_map(corpus, tm::removeNumbers)
    # - remove extra white spaces and trim (final step)
    corpus <- tm::tm_map(corpus, tm::stripWhitespace)
    corpus <- tm::tm_map(corpus, trans_gsub, "^\\s+|\\s+$", "")
}

# - define n-gram tokenizers
bigram_tokenizer <- function(x) unlist(
    lapply(NLP::ngrams(NLP::words(x), 2), paste, collapse = " "), use.names = FALSE)
trigram_tokenizer <- function(x) unlist(
    lapply(NLP::ngrams(NLP::words(x), 3), paste, collapse = " "), use.names = FALSE)
fourgram_tokenizer <- function(x) unlist(
    lapply(NLP::ngrams(NLP::words(x), 4), paste, collapse = " "), use.names = FALSE)

# - construct cleaned corpus
corpus_all <- clean_corpus(corpus_all)

# - construct term document matrices
tdm_all <- tm::TermDocumentMatrix(corpus_all)
tdm_all_bi <- tm::TermDocumentMatrix(corpus_all, control = list(tokenize = bigram_tokenizer))
tdm_all_tri <- tm::TermDocumentMatrix(corpus_all, control = list(tokenize = trigram_tokenizer))
```


### Exploratory Data Analysis
The following analysis is performed:  

* _Character Frequency_: each characters percentage of total  
* _Term Frequency_:  distribution of word counts  
* _Count Based Evaluation_: mark the n-grams with the highest occurrence  
* _Word Coverage_: percentage of n-grams needed to represent corresponding percentage of corpus  

```{r eda_char_freq, fig.align='center', fig.width=12}
# - dot plot of all characters in cleaned data set
data.frame(qdap::dist_tab(unlist(strsplit(rownames(tdm_all),"")))) %>%
    dplyr::arrange(desc(percent)) %>%
    ggplot(aes(x=reorder(interval,-percent),y=percent)) +
        geom_point(stat="identity") +
        labs(title="Character Frequency", x="character", y="percent")
```

```{r eda_term_freq, fig.align='center', fig.width=12}
# - define function to convert term document matrix to frequency data frame
get_freq_df <- function(tdm)
{
    freq <- rowSums(as.matrix(tdm))
    ttl_freq <- sum(freq)
    data.frame(ngram=names(freq), freq=freq) %>% 
        dplyr::arrange(desc(freq)) %>%
        dplyr::mutate(
            n = 1:n(),
            n_cum_pct = n / n(),
            freq_pct = ifelse(freq==0,0,freq/ttl_freq),
            freq_cum_pct = cumsum(freq_pct))
}

# - define function to perform term frequency analysis
freq_dist <- function(tdm, gt_filter=0, title="")
{
    get_freq_df(tdm) %>% 
        dplyr::filter(freq > gt_filter) %>%
        ggplot(aes(x=freq)) + 
            geom_histogram(aes(y = ..density..), alpha=0.6) + 
            geom_density() + 
            # - using base 10 (vs 2) since data is spread over a very large range
            scale_x_continuous(trans='log10') + 
            labs(title=title, x="frequency (log10)")
}

# - term frequency
p_gt0 <- freq_dist(tdm_all, gt_filter=0, title=paste("Term Frequency - All"))
p_gt100 <- freq_dist(tdm_all, gt_filter=100, title=paste("Term Frequency - Count >100"))
grid.arrange(p_gt0, p_gt100, ncol=2)
```

```{r eda_ngram_counts, fig.align='center', fig.width=12, fig.height=12}
# - define function to plot top-n n-grams in term document matrix
count_based_eval <- function(tdm, top_n=10, title="")
{
    get_freq_df(tdm) %>% 
        dplyr::top_n(top_n, freq) %>%
        # - explicitly limiting  results since dplyr::top_n will return more then n when there is a tie, 
        arrange(desc(freq)) %>% filter(1:n()<=top_n) %>%
        ggplot(aes(x=reorder(ngram,freq),y=freq)) + 
            geom_point(stat="identity") + 
            coord_flip() +
            labs(title=title) + 
            theme(axis.title = element_blank(),
                panel.grid.major.x = element_blank(),
                panel.grid.major.y = element_line(linetype=3, color="darkgray"))
}

# - plot ngram frequencies
top_n <- 20
p_count_1grams <- count_based_eval(tdm_all, top_n=top_n, title=paste("Top",top_n,"Unigrams"))
p_count_2grams <- count_based_eval(tdm_all_bi, top_n=top_n, title=paste("Top",top_n,"Bigrams"))
p_count_3grams <- count_based_eval(tdm_all_tri, top_n=top_n, title=paste("Top",top_n,"Trigrams"))
grid.arrange(p_count_1grams, p_count_2grams, p_count_3grams, nrow=3)
```

```{r eda_ngram_cov, fig.align='center', fig.width=12}
# - define function to compute percentage of corpus covered
ngram_coverage <- function(df_freq, title="", ymark=0.9)
{
    # - find row closest to ymark
    df_pt <- df_freq %>% dplyr::mutate(quant = abs(freq_cum_pct-ymark)) %>% dplyr::filter(rank(quant)==1)
    # - construct cumulative sum plot
    ggplot2::ggplot(df_freq, aes(x=n_cum_pct, y=freq_cum_pct)) + 
        geom_line() +
        geom_point(data=df_pt, aes(x=n_cum_pct, y=freq_cum_pct)) + 
        geom_vline(xintercept=df_pt$n_cum_pct[1], linetype="dashed", alpha=0.5) +
        geom_hline(yintercept=df_pt$freq_cum_pct[1], linetype="dashed", alpha=0.5) +
        scale_y_continuous(breaks=c(seq(0,1,by=0.5),ymark)) + 
        scale_x_continuous(breaks=c(seq(0,1,by=0.5),round(df_pt$n_cum_pct[1],2))) + 
        labs(title=title, x="word count", y="cumulative percentage")   
}

# - plot cumulative frequency for n-grams
p_cov_1gram <- ngram_coverage(get_freq_df(tdm_all), "Unigram Coverage")
p_cov_2gram <- ngram_coverage(get_freq_df(tdm_all_bi), "Bigram Coverage")
p_cov_3gram <- ngram_coverage(get_freq_df(tdm_all_tri), "Trigram Coverage")
grid.arrange(p_cov_1gram, p_cov_2gram, p_cov_3gram, ncol=3)
```


### Future Development
_Prediction Algorithm_: build a language model such that given a sequence of words the algorithm will predict the next most likely word. For example, given the word order w1, w2, w3 the algorithm will suggest the most probable next word w4.  

_Shiny app_: build a Shiny app that allows users to interact with the prediction algorithm. The app will accept an n-gram and predict the next word, with the highest probability, for the user.  

